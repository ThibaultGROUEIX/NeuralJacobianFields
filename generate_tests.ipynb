{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Diffusionnet nopt: hyperparameter experiments for balancing cuts vs distortion optimization\n",
    "basestr = [\"--experiment_type\", \"DEFAULT\", \"--size_train\", \"1\", \"--size_test\", \"1\", \"--epochs\", \"20000\",\n",
    "             \"--val_interval\", \"50\", \"--align_2D\", \"--xp_type\", \"uv\", \"--gpu_strategy\", \"ddp\",\n",
    "              \"--n_gpu\", \"1\", \"--no_poisson\", \"--noencoder\", \"--lr\", \"1e-4\",\n",
    "              \"--targets_per_batch\", \"16\", \"--workers\", \"0\", \"--valrenderratio\", \"1\",\n",
    "              \"--stitchingloss\", \"edgecutloss\", \"--accumulate_grad_batches\", \"1\", \"--outputdir\", \"./outputs/neuralopt/softpoisson\",\n",
    "              \"--identity\", \"--facedim\", \"3\", \"--seamlessedgecut\", \"--seamlessdelta\", \"0.0005\",\n",
    "              \"--spweight\", \"sigmoid\", \"--initweightinput\", \"--initjinput\",\n",
    "              \"--projectname\", \"conetest_nopt\", \"--cuteps\", \"5e-2\",\n",
    "              ]\n",
    "basestr = \"python ./training_scripts/train.py \" + \" \".join(basestr)\n",
    "ignore_edges = [298, 464, 555, 301, 304, 605, 456, 46,717,552,700,699,692, 691,\n",
    "                647, 190, 16, 200, 761, 757, 342, 662, 577, 122, 510, 79, 20, 193]\n",
    "\n",
    "baseoutput = \"./outputs/neuralopt/softpoisson\"\n",
    "datadirs = ['cone2']\n",
    "distortionlosses = ['arap']\n",
    "edgeweights = [0.1]\n",
    "stitchschedule = ['linear', 'constant']\n",
    "inits = ['tutte', 'isometric']\n",
    "ckptdir = \"/net/scratch/rliu/NJFWand/outputs/neuralopt/softpoisson/cone_eweight0_stitchNone/ckpt\"\n",
    "\n",
    "f = open(\"./slurm/cone2_stitchsched\", 'w')\n",
    "for data in datadirs:\n",
    "    for init in inits:\n",
    "        for schedule in stitchschedule:\n",
    "            for eweight in edgeweights:\n",
    "                if eweight == 0 and schedule is not None:\n",
    "                    continue\n",
    "                if schedule == 'constant' and init == 'isometric':\n",
    "                    continue\n",
    "                expstr = basestr + f\" --root_dir_train ./data/{data} --root_dir_test ./data/{data} --data_file {data}.json\"\n",
    "                expstr += f\" --spweight softmax --softpoisson edges --sparsecutsloss --sparsecuts_weight 0.1 --arch diffusionnet --init {init}\"\n",
    "                expstr += f\" --lossdistortion arap --arapnorm --edgecut_weight {eweight} --edgecut_weight_max {eweight} --edgecut_weight_min 0 --ninit 1\"\n",
    "\n",
    "                if schedule:\n",
    "                    expstr += f\" --stitchschedule {schedule}\"\n",
    "                if schedule == \"constant\":\n",
    "                    expstr += f\" --stitchschedule_constant 0.5\"\n",
    "\n",
    "                expname = f\"softmax_{data}_init{init}_eweight{eweight}_stitch{schedule}\"\n",
    "\n",
    "                expstr += f\" --expname {expname}\"\n",
    "                f.write(expstr + \"\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Diffusionnet nopt: hyperparameter experiments for balancing cuts vs distortion optimization\n",
    "basestr = [\"--experiment_type\", \"DEFAULT\", \"--size_train\", \"1\", \"--size_test\", \"1\", \"--epochs\", \"20000\",\n",
    "             \"--val_interval\", \"50\", \"--align_2D\", \"--xp_type\", \"uv\", \"--gpu_strategy\", \"ddp\",\n",
    "              \"--n_gpu\", \"1\", \"--no_poisson\", \"--noencoder\", \"--lr\", \"1e-4\",\n",
    "              \"--targets_per_batch\", \"16\", \"--workers\", \"0\", \"--valrenderratio\", \"1\",\n",
    "              \"--stitchingloss\", \"edgecutloss\", \"--accumulate_grad_batches\", \"1\", \"--outputdir\", \"./outputs/neuralopt/softpoisson\",\n",
    "              \"--identity\", \"--facedim\", \"3\", \"--seamlessedgecut\", \"--seamlessdelta\", \"0.0005\",\n",
    "              \"--spweight\", \"sigmoid\", \"--initweightinput\", \"--initjinput\",\n",
    "              \"--projectname\", \"conetest_nopt\", \"--cuteps\", \"5e-2\",\n",
    "              ]\n",
    "basestr = \"python ./training_scripts/train.py \" + \" \".join(basestr)\n",
    "ignore_edges = [298, 464, 555, 301, 304, 605, 456, 46,717,552,700,699,692, 691,\n",
    "                647, 190, 16, 200, 761, 757, 342, 662, 577, 122, 510, 79, 20, 193]\n",
    "\n",
    "baseoutput = \"./outputs/neuralopt/softpoisson\"\n",
    "datadirs = ['cone']\n",
    "distortionlosses = ['arap']\n",
    "edgeweights = [0.01, 0.05, 0.1]\n",
    "stitchschedule = ['linear']\n",
    "inits = ['isometric']\n",
    "ckptdir = \"/net/scratch/rliu/NJFWand/outputs/neuralopt/softpoisson/cone_eweight0_stitchNone/ckpt\"\n",
    "\n",
    "f = open(\"./slurm/cone_stitchsched\", 'w')\n",
    "for data in datadirs:\n",
    "    for init in inits:\n",
    "        for schedule in stitchschedule:\n",
    "            for eweight in edgeweights:\n",
    "                if eweight == 0 and schedule is not None:\n",
    "                    continue\n",
    "                if schedule == 'constant' and init == 'isometric':\n",
    "                    continue\n",
    "                expstr = basestr + f\" --root_dir_train ./data/{data} --root_dir_test ./data/{data} --data_file {data}.json\"\n",
    "                expstr += f\" --softpoisson edges --sparsecutsloss --sparsecuts_weight 0.1 --arch diffusionnet --init {init}\"\n",
    "                expstr += f\" --lossdistortion arap --arapnorm --edgecut_weight {eweight} --edgecut_weight_max {eweight} --edgecut_weight_min 0 --ninit 1\"\n",
    "\n",
    "                if schedule:\n",
    "                    expstr += f\" --stitchschedule {schedule}\"\n",
    "                if schedule == \"constant\":\n",
    "                    expstr += f\" --stitchschedule_constant 0.5\"\n",
    "\n",
    "                expname = f\"{data}_init{init}_eweight{eweight}_stitch{schedule}\"\n",
    "\n",
    "                expstr += f\" --expname {expname}\"\n",
    "                f.write(expstr + \"\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Diffusionnet cut learning with preset inits\n",
    "basestr = [\"--experiment_type\", \"DEFAULT\", \"--size_train\", \"1\", \"--size_test\", \"1\", \"--epochs\", \"25000\",\n",
    "             \"--val_interval\", \"50\", \"--align_2D\", \"--xp_type\", \"uv\", \"--gpu_strategy\", \"ddp\",\n",
    "              \"--n_gpu\", \"1\", \"--no_poisson\", \"--noencoder\",\n",
    "              \"--lossdistortion\", \"arap\", \"--lr\", \"1e-4\",\n",
    "              \"--targets_per_batch\", \"16\", \"--workers\", \"0\", \"--valrenderratio\", \"1\",\n",
    "              \"--stitchingloss\", \"edgecutloss\", \"--accumulate_grad_batches\", \"1\", \"--outputdir\", \"./outputs/learning/softpoisson\",\n",
    "              \"--identity\", \"--seamlessedgecut\", \"--seamlessdelta\", \"0.0005\",\n",
    "            \"--initweightinput\", \"--initjinput\",\n",
    "              \"--projectname\", \"cone_learn\", \"--cuteps\", \"5e-2\",\n",
    "              ]\n",
    "gradbatches = [1, 3, 8]\n",
    "softmaxs = [False, True]\n",
    "inits = ['tutte', 'isometric']\n",
    "datas = ['cone2']\n",
    "basestr = \"python ./training_scripts/train.py \" + \" \".join(basestr)\n",
    "\n",
    "f = open(\"./slurm/conetest_ninit\", 'w')\n",
    "for data in datas:\n",
    "    for removecut in [False]:\n",
    "        for softmax in softmaxs:\n",
    "            for batch in gradbatches:\n",
    "                for init in inits:\n",
    "                    for ninit in [-1]:\n",
    "                        expstr = basestr + f\" --init {init} --ninit {ninit} --root_dir_train ./data/{data} --root_dir_test ./data/{data} --data_file {data}.json\"\n",
    "                        expstr += f\" --softpoisson edges --sparsecutsloss --sparsecuts_weight 0.1 --arch diffusionnet\"\n",
    "                        expstr += f\" --min_cuts 4 --max_cuts 18 --simplecut --arapnorm --edgecut_weight 0.01\"\n",
    "                        expstr += f\" --accumulate_grad_batches {batch}\"\n",
    "                        expname = f\"{data}_gradbatch{batch}_softmax{softmax}_init{init}\"\n",
    "\n",
    "                        if init == 'isometric':\n",
    "                            expstr += \" --basistype rot\"\n",
    "\n",
    "                        if removecut:\n",
    "                            expstr += f\" --removecutfromloss\"\n",
    "                            expname += f\"_removecut\"\n",
    "                        if softmax:\n",
    "                            expstr += f\" --spweight softmax\"\n",
    "                        else:\n",
    "                            expstr += f\" --spweight sigmoid\"\n",
    "                        expstr += f\" --expname {expname}\"\n",
    "                        f.write(expstr + \"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Diffusionnet nopt experiments with random cuts just to sanity check the optimization\n",
    "basestr = [\"--experiment_type\", \"DEFAULT\", \"--size_train\", \"1\", \"--size_test\", \"1\", \"--epochs\", \"20000\",\n",
    "             \"--val_interval\", \"50\", \"--align_2D\", \"--xp_type\", \"uv\", \"--gpu_strategy\", \"ddp\",\n",
    "              \"--n_gpu\", \"1\", \"--no_poisson\", \"--noencoder\",\n",
    "              \"--lossdistortion\", \"arap\", \"--lr\", \"1e-4\",\n",
    "              \"--targets_per_batch\", \"16\", \"--workers\", \"0\", \"--valrenderratio\", \"1\",\n",
    "              \"--stitchingloss\", \"edgecutloss\", \"--accumulate_grad_batches\", \"1\", \"--outputdir\", \"./outputs/neuralopt/softpoisson\",\n",
    "              \"--identity\", \"--seamlessedgecut\", \"--seamlessdelta\", \"0.0005\",\n",
    "              \"--spweight\", \"sigmoid\", \"--initweightinput\", \"--initjinput\",\n",
    "              \"--projectname\", \"conetest_nopt\"\n",
    "              ]\n",
    "basestr = \"python ./training_scripts/train.py \" + \" \".join(basestr)\n",
    "\n",
    "f = open(\"./slurm/conetest_nopt_randcut\", 'w')\n",
    "for i in range(10):\n",
    "    expstr = basestr + f\" --init tutte --ninit 1 --root_dir_train ./data/cone --root_dir_test ./data/cone --data_file cone.json\"\n",
    "    expstr += f\" --softpoisson edges --sparsecutsloss --sparsecuts_weight 0.1 --arch diffusionnet --edgecut_weight 0.1\"\n",
    "    expstr += f\" --min_cuts 4 --max_cuts 16 --simplecut --removecutfromloss\"\n",
    "    expname = f\"cone_cut{i}\"\n",
    "\n",
    "    expstr += f\" --expname {expname}\"\n",
    "    f.write(expstr + \"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Diffusionnet nopt experiments with preset edge cutting\n",
    "# Don't remove the cut edges from the loss now\n",
    "basestr = [\"--experiment_type\", \"DEFAULT\", \"--size_train\", \"1\", \"--size_test\", \"1\", \"--epochs\", \"20000\",\n",
    "             \"--val_interval\", \"50\", \"--align_2D\", \"--xp_type\", \"uv\", \"--gpu_strategy\", \"ddp\",\n",
    "              \"--n_gpu\", \"1\", \"--no_poisson\", \"--noencoder\",\n",
    "            \"--lr\", \"1e-4\",\n",
    "              \"--targets_per_batch\", \"16\", \"--workers\", \"0\", \"--valrenderratio\", \"1\",\n",
    "              \"--stitchingloss\", \"edgecutloss\", \"--accumulate_grad_batches\", \"1\", \"--outputdir\", \"./outputs/neuralopt/softpoisson\",\n",
    "              \"--identity\", \"--facedim\", \"3\", \"--seamlessedgecut\", \"--seamlessdelta\", \"0.0005\",\n",
    "              \"--spweight\", \"sigmoid\", \"--initweightinput\", \"--initjinput\",\n",
    "              \"--projectname\", \"conetest_nopt\"\n",
    "              ]\n",
    "basestr = \"python ./training_scripts/train.py \" + \" \".join(basestr)\n",
    "ignore_edges = [298, 464, 555, 301, 304, 605, 456, 46,717,552,700,699,692, 691,\n",
    "                647, 190, 16, 200, 761, 757, 342, 662, 577, 122, 510, 79, 20, 193]\n",
    "\n",
    "baseoutput = \"./outputs/neuralopt/softpoisson\"\n",
    "datadirs = ['cone']\n",
    "ninits = [\"tutte\"]\n",
    "removecuts = [True, False]\n",
    "invjlosses = [True, False]\n",
    "arapnorms = [True, False]\n",
    "distortionlosses = ['arap']\n",
    "\n",
    "f = open(\"./slurm/conetest_nopt\", 'w')\n",
    "for arapnorm in arapnorms:\n",
    "    for removecut in removecuts:\n",
    "        for init in ninits:\n",
    "            for data in datadirs:\n",
    "                for invjloss in invjlosses:\n",
    "                    for i in range(4):\n",
    "                        expstr = basestr + f\" --root_dir_train ./data/{data} --root_dir_test ./data/{data} --data_file {data}.json\"\n",
    "                        expstr += f\" --softpoisson edges --sparsecutsloss --sparsecuts_weight 0.1 --arch diffusionnet\"\n",
    "                        expstr += f\" --lossdistortion arap\"\n",
    "\n",
    "                        if i > 0:\n",
    "                            idx = (len(ignore_edges)) // 3 * i + 1\n",
    "                            expstr += f\" --ignorei {idx}\"\n",
    "\n",
    "                        expname = f\"{data}\"\n",
    "\n",
    "                        if arapnorm:\n",
    "                            expstr += f\" --arapnorm\"\n",
    "                            expname += \"_arapnorm\"\n",
    "\n",
    "                        if init == 'isometric' and not removecut:\n",
    "                            continue\n",
    "                        elif init == \"isometric\":\n",
    "                            expstr += f\" --init isometric --ninit 0\"\n",
    "                            expname += f\"_initiso{i}\"\n",
    "                        elif init == \"tutte\" and i == 0:\n",
    "                            expstr += f\" --init tutte --ninit 0\"\n",
    "                            expname += f\"_inittutte{i}\"\n",
    "                        elif init == \"tutte\":\n",
    "                            expstr += f\" --init tutte --ninit 1\"\n",
    "                            expname += f\"_inittutte{i}\"\n",
    "\n",
    "                        if removecut:\n",
    "                            expstr += f\" --removecutfromloss\"\n",
    "                            expname += f\"_removecut\"\n",
    "\n",
    "                        if invjloss:\n",
    "                            expstr += \" --invjloss\"\n",
    "                            expname += f\"_invjloss\"\n",
    "\n",
    "                        expstr += f\" --expname {expname}\"\n",
    "                        f.write(expstr + \"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Neuralopt experiments with softpoisson\n",
    "basestr = [\"--experiment_type\", \"DEFAULT\", \"--size_train\", \"1\", \"--size_test\", \"1\", \"--epochs\", \"20000\",\n",
    "             \"--val_interval\", \"50\", \"--align_2D\", \"--xp_type\", \"uv\", \"--gpu_strategy\", \"ddp\",\n",
    "              \"--n_gpu\", \"1\", \"--no_poisson\", \"--noencoder\",\n",
    "              \"--lossdistortion\", \"arap\", \"--projectname\", \"sp_neuralopt\", \"--lr\", \"1e-5\",\n",
    "              \"--targets_per_batch\", \"16\", \"--workers\", \"0\", \"--valrenderratio\", \"1\",\n",
    "              \"--stitchingloss\", \"edgecutloss\", \"--accumulate_grad_batches\", \"1\", \"--outputdir\", \"./outputs/neuralopt/softpoisson\",\n",
    "              \"--identity\", \"--facedim\", \"3\", \"--seamlessedgecut\", \"--seamlessdelta\", \"0.0005\", \"--edgecut_weight\", \"1\",\n",
    "              \"--spweight\", \"cosine\", \"--initweightinput\", \"--initjinput\",\n",
    "              ]\n",
    "basestr = \"python ./training_scripts/train.py \" + \" \".join(basestr)\n",
    "\n",
    "baseoutput = \"./outputs/neuralopt/softpoisson\"\n",
    "datadirs = ['cylinder_nocut', 'cone', 'floatie', 'halfbunny', 'tetrahedron_sub1', 'tetrahedron_sub2']\n",
    "softpoissons = ['edges']\n",
    "inits = ['isometric']\n",
    "stitchweights = [\"softweightdetach\", \"stitchloss\"]\n",
    "sweight_schedule = ['linear', 'cosine']\n",
    "spweights = ['cosine']\n",
    "sparsepoisson = [True, False]\n",
    "\n",
    "f = open(\"./slurm/softpoisson_nopt\", 'w')\n",
    "for data in datadirs:\n",
    "    expstr = basestr + f\" --init isometric --ninit 1 --root_dir_train ./data/{data} --root_dir_test ./data/{data} --data_file {data}.json\"\n",
    "    expstr += f\" --softpoisson edges\"\n",
    "\n",
    "    expname = f\"neuralopt_{data}_initinputs\"\n",
    "    expstr += f\" --expname {expname}\"\n",
    "    f.write(expstr + \"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Learning experiments with diffusionnet sp\n",
    "basestr = [\"--experiment_type\", \"DEFAULT\", \"--size_train\", \"1\", \"--size_test\", \"1\", \"--epochs\", \"10000\",\n",
    "             \"--val_interval\", \"50\", \"--align_2D\", \"--xp_type\", \"uv\", \"--gpu_strategy\", \"ddp\",\n",
    "              \"--n_gpu\", \"1\", \"--no_poisson\", \"--noencoder\",\n",
    "              \"--projectname\", \"softpoisson\", \"--lr\", \"1e-4\",\n",
    "              \"--targets_per_batch\", \"16\", \"--workers\", \"0\", \"--valrenderratio\", \"1\",\n",
    "              \"--accumulate_grad_batches\", \"1\", \"--outputdir\", \"./outputs/learning/softpoisson\",\n",
    "              \"--initweightinput\", \"--initjinput\", \"--facedim\", \"128\",\n",
    "              \"--identity\", \"--seamlessedgecut\", \"--seamlessdelta\", \"0.0005\", \"--edgecut_weight\", \"1\",\n",
    "              ]\n",
    "basestr = \"python ./training_scripts/train.py \" + \" \".join(basestr)\n",
    "\n",
    "baseoutput = \"./outputs/learning/softpoisson\"\n",
    "datadirs = ['cone']\n",
    "softpoissons = ['edges']\n",
    "inits = ['tutte', 'slim', 'isometric']\n",
    "\n",
    "f = open(\"./slurm/softpoisson_simple_gt\", 'w')\n",
    "for data in datadirs:\n",
    "    for poisson in softpoissons:\n",
    "        for init in inits:\n",
    "            expstr = basestr + f\" --init {init} --ninit -1 --root_dir_train ./data/{data} --root_dir_test ./data/{data} --data_file {data}.json\"\n",
    "            expstr += f\" --softpoisson {poisson} --simplecut --arch diffusionnet --spweight sigmoid\"\n",
    "            expstr += \" --gtuvloss\"\n",
    "            # expstr += \" --stitchingloss edgecutloss --lossdistortion arap --sparsecutsloss\"\n",
    "\n",
    "            if data == \"floatie\":\n",
    "                expstr += f\" --min_cuts 2 --max_cuts 20\"\n",
    "            elif data == \"cone\":\n",
    "                expstr += f\" --min_cuts 0 --max_cuts 12\"\n",
    "            elif data == \"tetrahedron_sub2\":\n",
    "                expstr += f\" --min_cuts 0 --max_cuts 2 --top_k_eig 8 \"\n",
    "            else:\n",
    "                expstr += f\" --min_cuts 0 --max_cuts 20\"\n",
    "\n",
    "            if init == \"slim\":\n",
    "                expstr += f\" --slimiters 2\"\n",
    "            elif init == \"isometric\":\n",
    "                expstr += f\" --basistype rot\"\n",
    "\n",
    "            expname = f\"gtcutlearn_{data}_init{init}\"\n",
    "            expstr += f\" --expname {expname}\"\n",
    "            f.write(expstr + \"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "### Soft poisson direct opt\n",
    "basestr = \"python opt_softpoisson.py \"\n",
    "datadirs = ['./data/tetrahedron4.obj', \"./data/tetrahedron_sub1.obj\", \"./data/tetrahedron_sub2.obj\"]\n",
    "stitchweight = ['softweightdetach']\n",
    "distortionlosses = ['symmetricdirichlet', 'arap']\n",
    "edgecuts = ['seamless', 'vertexsep']\n",
    "edgeweights = ['seploss', 'softweightdetach']\n",
    "\n",
    "f = open(\"slurm/softpoisson_opt\", 'w')\n",
    "for data in datadirs:\n",
    "    for distortionloss in distortionlosses:\n",
    "        for edgecut in edgecuts:\n",
    "            for edgeweight in edgeweights:\n",
    "                objname = os.path.basename(data).split(\".\")[0]\n",
    "                savename = f\"{objname}_ecut{edgecut}_eweight{edgeweight}_dist{distortionloss}\"\n",
    "                expstr = basestr + f\" --objdir {data} --niters 300000 --init isometric --savename {savename} --lr 1e-4\"\n",
    "                expstr += f\" --distortionloss {distortionloss} --edgeweight {edgeweight} --edgecutloss {edgecut}\"\n",
    "                expstr += f\" --seplossdelta 0.01 --stitchweight softweightdetach \"\n",
    "                expstr += f\" --vertexseptype l2 --vertexsepsqrt\"\n",
    "            f.write(expstr + \"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "### Soft poisson direct opt\n",
    "basestr = \"python opt_softpoisson.py \"\n",
    "datadirs = ['./data/tetrahedron4.obj', './data/tetrahedron5.obj', \"./data/tetrahedron_sub1.obj\", \"./data/tetrahedron_sub2.obj\"]\n",
    "stitchweight = ['softweight', 'softweightdetach', 'seploss']\n",
    "seamlesses = [True, False]\n",
    "vseptype = ['l2sqrt']\n",
    "\n",
    "f = open(\"slurm/softpoisson_opt\", 'w')\n",
    "for data in datadirs:\n",
    "    for seamless in seamlesses:\n",
    "        for sweight in stitchweight:\n",
    "            objname = os.path.basename(data).split(\".\")[0]\n",
    "            savename = f\"{objname}_edgecut_seamless{seamless}_sweight{sweight}\"\n",
    "            expstr = basestr + f\" --objdir {data} --niters 300000 --init isometric --vertexseploss --symmetricdirichletloss --savename {savename} --lr 1e-4\"\n",
    "            expstr += f\" --seplossdelta 0.01\"\n",
    "            expstr += f\" --vertexseptype l2 --vertexsepsqrt --edgecutloss seamless --stitchweight {sweight}\"\n",
    "\n",
    "            if seamless:\n",
    "                expstr += f\" --seamless\"\n",
    "\n",
    "            if sweight == \"seploss\":\n",
    "                expstr += f\" --niters 1000000 --lr 1e-3\"\n",
    "\n",
    "            f.write(expstr + \"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "### Soft poisson direct opt\n",
    "basestr = \"python opt_softpoisson.py \"\n",
    "datadirs = ['./data/tetrahedron.obj', './data/tetrahedron4.obj', \"./data/tetrahedron_sub1.obj\", \"./data/tetrahedron_sub2.obj\"]\n",
    "weightlosses = [True, False]\n",
    "seplossdeltas = [0.01, 0.001, 0.0001]\n",
    "vseptype = ['l2sqrt', 'l2']\n",
    "\n",
    "f = open(\"slurm/softpoisson_opt\", 'w')\n",
    "for data in datadirs:\n",
    "    for vsep in vseptype:\n",
    "        for delta in seplossdeltas:\n",
    "            objname = os.path.basename(data).split(\".\")[0]\n",
    "            savename = f\"{objname}_septype{vsep}_sepdelta{delta}\"\n",
    "            expstr = basestr + f\" --objdir {data} --niters 300000 --init isometric --vertexseploss --symmetricdirichletloss --savename {savename}\"\n",
    "            if vsep == 'l2sqrt':\n",
    "                expstr += f\" --lr 1e-4 --vertexseptype l2 --seamless --seplossdelta {delta} --vertexsepsqrt\"\n",
    "            else:\n",
    "                expstr += f\" --lr 1e-4 --vertexseptype {vsep} --seamless --seplossdelta {delta}\"\n",
    "            f.write(expstr + \"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "### Soft poisson direct opt\n",
    "basestr = \"python opt_softpoisson.py \"\n",
    "datadirs = ['./data/floatie/floatie.obj', './data/halfbunny/halfbunny.obj', \"./data/coarsecylinder_nocut/coarsecylinder_nocut.obj\"]\n",
    "weightlosses = [True, False]\n",
    "seplossdeltas = [0.01, 0.001]\n",
    "\n",
    "f = open(\"slurm/softpoisson_opt\", 'w')\n",
    "for data in datadirs:\n",
    "    for wloss in weightlosses:\n",
    "        for seplossdelta in seplossdeltas:\n",
    "            objname = os.path.basename(data).split(\".\")[0]\n",
    "            savename = f\"{objname}_wloss{wloss}_sepdelta{seplossdelta}\"\n",
    "            expstr = basestr + f\" --objdir {data} --niters 300000 --init isometric --vertexseploss --symmetricdirichletloss --seplossdelta {seplossdelta} --savename {savename}\"\n",
    "            if wloss:\n",
    "                expstr += f\" --weightloss\"\n",
    "            f.write(expstr + \"\\n\")\n",
    "\n",
    "# Also try init tutte for coarsecylinder\n",
    "data = \"./data/coarsecylinder_nocut/coarsecylinder_nocut.obj\"\n",
    "for wloss in weightlosses:\n",
    "    for seplossdelta in seplossdeltas:\n",
    "        objname = os.path.basename(data).split(\".\")[0]\n",
    "        savename = f\"{objname}_wloss{wloss}_sepdelta{seplossdelta}_initslim\"\n",
    "        expstr = basestr + f\" --objdir {data} --niters 300000 --init slim --vertexseploss --symmetricdirichletloss --seplossdelta {seplossdelta} --savename {savename}\"\n",
    "        if wloss:\n",
    "            expstr += f\" --weightloss\"\n",
    "        f.write(expstr + \"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Neural optimization: try cut cylinder with translation optimization (no random translation of isometric)\n",
    "basestr = [\"--experiment_type\", \"DEFAULT\", \"--size_train\", \"1\", \"--size_test\", \"1\", \"--epochs\", \"20000\",\n",
    "             \"--val_interval\", \"20\", \"--align_2D\", \"--xp_type\", \"uv\", \"--gpu_strategy\", \"ddp\",\n",
    "              \"--n_gpu\", \"1\", \"--no_poisson\", \"--identity\", \"--lr\", \"1e-5\", \"--ninit\", \"1\", \"--initjinput\",\n",
    "              \"--targets_per_batch\", \"16\", \"--workers\", \"8\", \"--outputdir\", \"./outputs/neuralopt\",\n",
    "              \"--accumulate_grad_batches\", \"1\", \"--projectname\", \"neuralopt\"]\n",
    "basestr = \"python ./training_scripts/train.py \" + ' '.join(basestr)\n",
    "datadirs = ['cylinder']\n",
    "l0relaxes = [True, False]\n",
    "losses = ['cosine', 'l2', 'l1']\n",
    "\n",
    "f = open(\"slurm/cylinder_trans\", 'w')\n",
    "for data in datadirs:\n",
    "    for l0 in l0relaxes:\n",
    "        expname = f\"trans_{data}_l0{l0}\"\n",
    "        expstr = basestr + f\" --init isometric --lossedgeseparation --root_dir_train ./data/{data} --root_dir_test ./data/{data} --expname {expname}\"\n",
    "        expstr += f\" --lossdistortion dirichlet --data_file {data}.json\"\n",
    "        if l0:\n",
    "            expstr += f\" --seploss_schedule --stitchrelax\"\n",
    "        f.write(expstr + \"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Neural optimization: try different shapes\n",
    "basestr = [\"--experiment_type\", \"DEFAULT\", \"--size_train\", \"1\", \"--size_test\", \"1\", \"--epochs\", \"20000\",\n",
    "             \"--val_interval\", \"20\", \"--align_2D\", \"--xp_type\", \"uv\", \"--gpu_strategy\", \"ddp\",\n",
    "              \"--n_gpu\", \"1\", \"--no_poisson\", \"--identity\", \"--lr\", \"1e-5\", \"--ninit\", \"1\", \"--initjinput\",\n",
    "              \"--targets_per_batch\", \"16\", \"--workers\", \"8\", \"--outputdir\", \"./outputs/neuralopt\",\n",
    "              \"--accumulate_grad_batches\", \"1\", \"--opttrans\", \"--projectname\", \"neuralopt\"]\n",
    "basestr = \"python ./training_scripts/train.py \" + ' '.join(basestr)\n",
    "inits = ['isometric', 'tutte']\n",
    "datadirs = ['cylinder_nocut', \"halfbunny\", \"cone\", \"floatie\", \"headphoneholder\", \"squirrel\"]\n",
    "distortions = ['dirichlet']\n",
    "l0relaxes = [True, False]\n",
    "losses = ['cosine', 'l2', 'l1']\n",
    "\n",
    "f = open(\"slurm/neuralopt_exps\", 'w')\n",
    "for data in datadirs:\n",
    "    for l0 in l0relaxes:\n",
    "        for loss in losses:\n",
    "            expname = f\"{data}_l0{l0}_loss{loss}\"\n",
    "            expstr = basestr + f\" --init isometric --lossgradientstitching {loss} --root_dir_train ./data/{data} --root_dir_test ./data/{data} --expname {expname}\"\n",
    "            expstr += f\" --lossdistortion dirichlet --data_file {data}.json\"\n",
    "            if l0:\n",
    "                expstr += f\" --seploss_schedule --stitchrelax\"\n",
    "            f.write(expstr + \"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Coordinate MLP (GT loss)\n",
    "basestr = [\"--experiment_type\", \"DEFAULT\", \"--size_train\", \"1\", \"--size_test\", \"1\", \"--epochs\", \"10000\",\n",
    "             \"--val_interval\", \"20\", \"--data_file\", \"cylinder.json\", \"--align_2D\", \"--xp_type\", \"uv\", \"--gpu_strategy\", \"ddp\",\n",
    "              \"--n_gpu\", \"1\", \"--no_poisson\", \"--identity\", \"--initjinput\", \"--noencoder\",\n",
    "              \"--projectname\", \"gtcyl\", \"--lr\", \"1e-4\", \"--lossgt\",\n",
    "              \"--targets_per_batch\", \"16\", \"--workers\", \"1\", \"--outputdir\", \"./outputs/learning/gtcylinder\",\n",
    "              \"--accumulate_grad_batches\", \"1\", \"--min_cuts\", \"2\", \"--max_cuts\", \"8\",\n",
    "              \"--top_k_eig\", \"30\"]\n",
    "\n",
    "basestr = \"python ./training_scripts/train.py \" + ' '.join(basestr)\n",
    "inits = ['tutte', 'isometric', 'slim']\n",
    "isometric_bases = ['basis', 'rot']\n",
    "slim_iters = [50, 100]\n",
    "usedense = [True, False]\n",
    "datadirs = ['coarsecylinder_nocut']\n",
    "\n",
    "f = open(\"./slurm/gt_cylinder\", 'w')\n",
    "for data in datadirs:\n",
    "    for init in inits:\n",
    "        expname = f\"gtcyl_{data}_init{init}\"\n",
    "        if init == \"isometric\":\n",
    "            for basis in isometric_bases:\n",
    "                expname += f\"_basis{basis}\"\n",
    "                expstr = basestr + f\" --init {init} --ninit -1 --root_dir_train ./data/{data} --root_dir_test ./data/{data} --expname {expname} --basistype {basis}\"\n",
    "                f.write(expstr + \"\\n\")\n",
    "        elif init == \"slim\":\n",
    "            for iter in slim_iters:\n",
    "                expname += f\"_iter{iter}\"\n",
    "                expstr = basestr + f\" --init {init} --ninit -1 --root_dir_train ./data/{data} --root_dir_test ./data/{data} --expname {expname} --slimiters {iter}\"\n",
    "                f.write(expstr + \"\\n\")\n",
    "        else:\n",
    "            expstr = basestr + f\" --init {init} --ninit -1 --root_dir_train ./data/{data} --root_dir_test ./data/{data} --expname {expname}\"\n",
    "            f.write(expstr + \"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Coordinate MLP (SLIM cutting on bunny)\n",
    "basestr = [\"--experiment_type\", \"DEFAULT\", \"--size_train\", \"1\", \"--size_test\", \"1\", \"--epochs\", \"10000\",\n",
    "             \"--val_interval\", \"20\", \"--align_2D\", \"--xp_type\", \"uv\", \"--gpu_strategy\", \"ddp\",\n",
    "              \"--n_gpu\", \"1\", \"--no_poisson\", \"--identity\", \"--initjinput\", \"--noencoder\",\n",
    "              \"--lossdistortion\", \"dirichlet\", \"--projectname\", \"cutlearning\", \"--lr\", \"1e-5\",\n",
    "              \"--targets_per_batch\", \"16\", \"--workers\", \"1\", \"--outputdir\", \"./outputs/learning/halfbunny\",\n",
    "              \"--accumulate_grad_batches\", \"1\", \"--opttrans\", \"--min_cuts\", \"10\", \"--max_cuts\", \"20\",\n",
    "              \"--lossgradientstitching\", \"cosine\"]\n",
    "\n",
    "# TODO: opttrans only for gradient stitching -- debug the translation component for euclidean loss\n",
    "basestr = \"python ./training_scripts/train.py \" + ' '.join(basestr)\n",
    "inits = ['tutte', 'slim']\n",
    "slimiters = [\"50\", \"100\", \"500\"]\n",
    "relaxstitches = [True, False]\n",
    "losstype = ['gradient']\n",
    "datadirs = ['halfbunny']\n",
    "lrs = ['1e-5']\n",
    "\n",
    "f = open(\"./slurm/bunny_mlp\", 'w')\n",
    "for data in datadirs:\n",
    "    for stitch in relaxstitches:\n",
    "        for init in inits:\n",
    "            expstr = basestr + f\" --init {init} --ninit -1 --root_dir_train ./data/{data} --root_dir_test ./data/{data} --data_file {data}.json\"\n",
    "            if stitch:\n",
    "                expstr += \" --stitchrelax\"\n",
    "\n",
    "            if init == \"tutte\":\n",
    "                expname = f\"coordmlp_{data}_init{init}_stitch{stitch}\"\n",
    "                expstr += f\" --expname {expname}\"\n",
    "\n",
    "                f.write(expstr + \"\\n\")\n",
    "\n",
    "            elif init == \"slim\":\n",
    "                for iter in slimiters:\n",
    "                    expname = f\"coordmlp_{data}_init{init}_stitch{stitch}_slim{iter}\"\n",
    "                    expstr += f\" --slimiters {iter} --expname {expname}\"\n",
    "\n",
    "                    f.write(expstr + \"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Dense MLP (Tutte cutting)\n",
    "basestr = [\"--experiment_type\", \"DEFAULT\", \"--size_train\", \"1\", \"--size_test\", \"1\", \"--epochs\", \"10000\",\n",
    "             \"--val_interval\", \"20\", \"--data_file\", \"cylinder.json\", \"--align_2D\", \"--xp_type\", \"uv\", \"--gpu_strategy\", \"ddp\",\n",
    "              \"--n_gpu\", \"1\", \"--no_poisson\", \"--identity\", \"--initjinput\", \"--noencoder\",\n",
    "              \"--lossdistortion\", \"dirichlet\", \"--projectname\", \"densetutte\", \"--lr\", \"1e-5\",\n",
    "              \"--targets_per_batch\", \"16\", \"--workers\", \"1\", \"--outputdir\", \"./outputs/learning/dense_tutte\",\n",
    "              \"--accumulate_grad_batches\", \"1\", \"--opttrans\", \"--min_cuts\", \"1\", \"--max_cuts\", \"6\",\n",
    "              \"--lossgradientstitching\", \"cosine\", \"--dense\", \"input\",\n",
    "              \"--top_k_eig\", \"40\"]\n",
    "\n",
    "basestr = \"python ./training_scripts/train.py \" + ' '.join(basestr)\n",
    "inits = ['tutte']\n",
    "datadirs = ['coarsecylinder', 'coarsecylinder_nocut']\n",
    "lrs = ['1e-5']\n",
    "\n",
    "f = open(\"./slurm/cylinder_dense_tutte\", 'w')\n",
    "for data in datadirs:\n",
    "    for init in inits:\n",
    "        expname = f\"dense_{data}_init{init}\"\n",
    "        expstr = basestr + f\" --init {init} --ninit -1 --root_dir_train ./data/{data} --root_dir_test ./data/{data} --expname {expname}\"\n",
    "        f.write(expstr + \"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Coordinate MLP (Tutte cutting)\n",
    "basestr = [\"--experiment_type\", \"DEFAULT\", \"--size_train\", \"1\", \"--size_test\", \"1\", \"--epochs\", \"10000\",\n",
    "             \"--val_interval\", \"20\", \"--data_file\", \"cylinder.json\", \"--align_2D\", \"--xp_type\", \"uv\", \"--gpu_strategy\", \"ddp\",\n",
    "              \"--n_gpu\", \"1\", \"--no_poisson\", \"--identity\", \"--initjinput\", \"--noencoder\",\n",
    "              \"--lossdistortion\", \"dirichlet\", \"--projectname\", \"coarsetutte\", \"--lr\", \"1e-5\",\n",
    "              \"--targets_per_batch\", \"16\", \"--workers\", \"1\", \"--outputdir\", \"./outputs/learning/coarse_tutte\",\n",
    "              \"--accumulate_grad_batches\", \"1\", \"--opttrans\", \"--min_cuts\", \"1\", \"--max_cuts\", \"6\",\n",
    "              \"--lossgradientstitching\", \"cosine\",\n",
    "              \"--top_k_eig\", \"40\"]\n",
    "\n",
    "basestr = \"python ./training_scripts/train.py \" + ' '.join(basestr)\n",
    "inits = ['tutte']\n",
    "datadirs = ['coarsecylinder', 'coarsecylinder_nocut']\n",
    "lrs = ['1e-5']\n",
    "\n",
    "f = open(\"./slurm/cylinder_coarse_tutte\", 'w')\n",
    "for data in datadirs:\n",
    "    for init in inits:\n",
    "        expname = f\"coordmlp_{data}_init{init}\"\n",
    "        expstr = basestr + f\" --init {init} --ninit -1 --root_dir_train ./data/{data} --root_dir_test ./data/{data} --expname {expname}\"\n",
    "        f.write(expstr + \"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Dense MLP\n",
    "## data type\n",
    "## stitching type\n",
    "## global rotation sampling/random/basis sampling\n",
    "basestr = [\"--experiment_type\", \"DEFAULT\", \"--size_train\", \"1\", \"--size_test\", \"1\", \"--epochs\", \"10000\",\n",
    "             \"--val_interval\", \"20\", \"--data_file\", \"cylinder.json\", \"--align_2D\", \"--xp_type\", \"uv\", \"--gpu_strategy\", \"ddp\",\n",
    "              \"--n_gpu\", \"1\", \"--no_poisson\", \"--identity\", \"--initjinput\", \"--noencoder\",\n",
    "              \"--lossdistortion\", \"dirichlet\", \"--projectname\", \"dense\", \"--lr\", \"1e-5\",\n",
    "              \"--targets_per_batch\", \"16\", \"--workers\", \"1\", \"--outputdir\", \"./outputs/dense_v2\",\n",
    "              \"--accumulate_grad_batches\", \"1\", \"--opttrans\",\n",
    "              \"--dense\", \"input\", \"--lossgradientstitching\", \"cosine\",\n",
    "              \"--top_k_eig\", \"40\"]\n",
    "\n",
    "basestr = \"python ./training_scripts/train.py \" + ' '.join(basestr)\n",
    "inits = ['isometric']\n",
    "basistypes = ['rot', 'basis', 'global']\n",
    "gradclips = [True, False]\n",
    "datadirs = ['coarsecylinder', 'coarsecylinder_nocut']\n",
    "lrs = ['1e-5']\n",
    "\n",
    "f = open(\"./slurm/cylinder_dense\", 'w')\n",
    "for data in datadirs:\n",
    "    for init in inits:\n",
    "        for basis in basistypes:\n",
    "            expname = f\"{data}_init{init}_basis{basis}\"\n",
    "            expstr = basestr + f\" --init {init} --ninit -1 --root_dir_train ./data/{data} --root_dir_test ./data/{data} --expname {expname}\"\n",
    "            expstr += f\" --basistype {basis}\"\n",
    "            f.write(expstr + \"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Coordinate MLP\n",
    "## data type\n",
    "## stitching type\n",
    "## global rotation sampling/random/basis sampling\n",
    "basestr = [\"--experiment_type\", \"DEFAULT\", \"--size_train\", \"1\", \"--size_test\", \"1\", \"--epochs\", \"10000\",\n",
    "             \"--val_interval\", \"20\", \"--data_file\", \"cylinder.json\", \"--align_2D\", \"--xp_type\", \"uv\", \"--gpu_strategy\", \"ddp\",\n",
    "              \"--n_gpu\", \"1\", \"--no_poisson\", \"--identity\", \"--initjinput\", \"--noencoder\",\n",
    "              \"--lossdistortion\", \"dirichlet\", \"--projectname\", \"dense\", \"--lr\", \"1e-5\",\n",
    "              \"--targets_per_batch\", \"16\", \"--workers\", \"1\", \"--outputdir\", \"./outputs/coordmlp\",\n",
    "              \"--accumulate_grad_batches\", \"1\", \"--opttrans\",\n",
    "              \"--lossgradientstitching\", \"cosine\",\n",
    "              \"--top_k_eig\", \"40\"]\n",
    "\n",
    "basestr = \"python ./training_scripts/train.py \" + ' '.join(basestr)\n",
    "inits = ['isometric']\n",
    "basistypes = ['rot', 'basis', 'global']\n",
    "gradclips = [True, False]\n",
    "datadirs = ['coarsecylinder', 'coarsecylinder_nocut']\n",
    "lrs = ['1e-5']\n",
    "\n",
    "f = open(\"./slurm/cylinder_coarse\", 'w')\n",
    "for data in datadirs:\n",
    "    for init in inits:\n",
    "        for basis in basistypes:\n",
    "            expname = f\"coordmlp_{data}_init{init}_basis{basis}\"\n",
    "            expstr = basestr + f\" --init {init} --ninit -1 --root_dir_train ./data/{data} --root_dir_test ./data/{data} --expname {expname}\"\n",
    "            expstr += f\" --basistype {basis}\"\n",
    "            f.write(expstr + \"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Hyperparam search: neural optimization w/ initj input\n",
    "## - distortion loss: arap/dirichlet\n",
    "## - initialization: tutte, isometric\n",
    "## - stitchloss schedule: T/F (tutte: 0->1, isometric: 1->2)\n",
    "## - l0relaxation + schedule: T/F\n",
    "\n",
    "basestr = [\"--experiment_type\", \"DEFAULT\", \"--size_train\", \"1\", \"--size_test\", \"1\", \"--epochs\", \"20000\",\n",
    "             \"--val_interval\", \"20\", \"--data_file\", \"cylinder.json\", \"--align_2D\", \"--xp_type\", \"uv\", \"--gpu_strategy\", \"ddp\",\n",
    "              \"--n_gpu\", \"1\", \"--no_poisson\", \"--identity\", \"--lr\", \"1e-5\", \"--ninit\", \"1\", \"--initjinput\",\n",
    "              \"--targets_per_batch\", \"16\", \"--workers\", \"8\", \"--outputdir\", \"./outputs/neuralopt\",\n",
    "              \"--accumulate_grad_batches\", \"1\", \"--opttrans\"]\n",
    "basestr = \"python ./training_scripts/train.py \" + ' '.join(basestr)\n",
    "inits = ['isometric', 'tutte']\n",
    "datadirs = ['cylinder', 'cylinder_nocut']\n",
    "distortions = ['dirichlet', 'arap']\n",
    "stitchschedules = [True, False]\n",
    "l0relaxes = [True, False]\n",
    "\n",
    "f = open(\"slurm/cylinder_neuralopt\", 'w')\n",
    "for data in datadirs:\n",
    "    for distortion in distortions:\n",
    "        for init in inits:\n",
    "            for stitchschedule in stitchschedules:\n",
    "                for l0 in l0relaxes:\n",
    "                    expname = f\"{data}_dist{distortion}_init{init}_stitchsched{stitchschedule}_l0{l0}\"\n",
    "                    expstr = basestr + f\" --init {init} --lossgradientstitching l2 --root_dir_train ./data/{data} --root_dir_test ./data/{data} --expname {expname}\"\n",
    "                    expstr += f\" --lossdistortion {distortion}\"\n",
    "                    if stitchschedule:\n",
    "                        if init == \"isometric\":\n",
    "                            expstr += f\" --stitchloss_schedule linear --stitchlossweight_min 1 --stitchlossweight_max 2\"\n",
    "                        if init == \"tutte\":\n",
    "                            expstr += f\" --stitchloss_schedule linear --stitchlossweight_min 0 --stitchlossweight_max 1\"\n",
    "                    if l0:\n",
    "                        expstr += f\" --seploss_schedule --stitchrelax\"\n",
    "                    f.write(expstr + \"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple init learning experiment\n",
    "basestr = [\"--experiment_type\", \"DEFAULT\", \"--size_train\", \"1\", \"--size_test\", \"1\", \"--epochs\", \"10000\",\n",
    "             \"--val_interval\", \"50\", \"--data_file\", \"cylinder.json\", \"--align_2D\", \"--xp_type\", \"uv\", \"--gpu_strategy\", \"ddp\",\n",
    "              \"--n_gpu\", \"1\", \"--no_poisson\", \"--identity\", \"--lr\", \"1e-5\", \"--workers\", \"8\", \"--opttrans\",\n",
    "              \"--lossdistortion\", \"dirichlet\", \"--targets_per_batch\", \"16\", \"--cuteps\", \"0.5\",\n",
    "              \"--accumulate_grad_batches\", \"1\", \"--initjinput\", \"--outputdir\", \"outputs/learning\"]\n",
    "\n",
    "basestr = \"python ./training_scripts/train.py \" + ' '.join(basestr)\n",
    "inits = ['isometric']\n",
    "losses = [None, 'l2']\n",
    "basistypes = ['basis', 'rot']\n",
    "# ninits = [1,2,5,10,20,100]\n",
    "ninit_to_ratio = {1:1, 2:1, 5:1, 10: 0.2, 20: 0.1, 100: 0.1}\n",
    "datadirs = ['cylinder', 'cylinder_nocut']\n",
    "\n",
    "f = open(\"slurm/cylinder_n\", 'w')\n",
    "for data in datadirs:\n",
    "    for init in inits:\n",
    "        for loss in losses:\n",
    "            for basis in basistypes:\n",
    "                expname = f\"{data}_init{init}_loss{loss}_basis{basis}\"\n",
    "                expstr = basestr + f\" --init {init} --ninit -1 --root_dir_train ./data/{data} --root_dir_test ./data/{data} --expname {expname}\"\n",
    "                expstr += f\" --basistype {basis}\"\n",
    "                # expstr += f\" --valrenderratio {ninit_to_ratio[ninit]}\"\n",
    "                if loss:\n",
    "                    expstr += f\" --lossgradientstitching {loss}\"\n",
    "                else:\n",
    "                    expstr += f\" --lossedgeseparation\"\n",
    "                f.write(expstr + \"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient stitching experiments\n",
    "basestr = [\"--experiment_type\", \"DEFAULT\", \"--size_train\", \"1\", \"--size_test\", \"1\", \"--epochs\", \"3000\",\n",
    "             \"--val_interval\", \"20\", \"--data_file\", \"cylinder.json\", \"--align_2D\", \"--xp_type\", \"uv\", \"--gpu_strategy\", \"ddp\",\n",
    "              \"--n_gpu\", \"1\", \"--no_poisson\", \"--identity\", \"--lr\", \"0.0001\",\n",
    "              \"--targets_per_batch\", \"16\",\n",
    "              \"--accumulate_grad_batches\", \"1\"]\n",
    "basestr = \"python ./training_scripts/train.py \" + ' '.join(basestr)\n",
    "# ffts = [True, False]\n",
    "inits = ['isometric', 'tutte']\n",
    "losses = ['l2']\n",
    "datadirs = ['cylinder', 'cylinder_nocut']\n",
    "distortions = ['dirichlet', 'edge']\n",
    "\n",
    "f = open(\"slurm/cylinder_grad_exps\", 'w')\n",
    "for data in datadirs:\n",
    "    for distortion in distortions:\n",
    "        for init in inits:\n",
    "            for loss in losses:\n",
    "                expname = f\"{data}_dist{distortion}_init{init}_loss{loss}\"\n",
    "                expstr = basestr + f\" --init {init} --lossgradientstitching {loss} --root_dir_train ./data/{data} --root_dir_test ./data/{data} --expname {expname}\"\n",
    "                # if fft:\n",
    "                #     expstr += \" --fft --fft_dim 256\"\n",
    "                expstr += f\" --lossdistortion {distortion}\"\n",
    "                f.write(expstr + \"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flat direct optimization experiments\n",
    "basestr = \"python directopt.py\"\n",
    "\n",
    "datadirs = ['cyl', 'cylnocut']\n",
    "inits = ['isom']\n",
    "# anneals = [True, False]\n",
    "grads = [None, 'l2', 'split']\n",
    "\n",
    "f = open(\"slurm/directopt_tests_v2\", 'w')\n",
    "for data in datadirs:\n",
    "    for init in inits:\n",
    "        for grad in grads:\n",
    "            # Don't anneal if grad\n",
    "            if anneal and grad is not None:\n",
    "                continue\n",
    "            expname = f\"dopt_v2_data{data}_init{init}_grad{grad}\"\n",
    "            expstr = basestr + f\" --vs ./scratch/data/{data}_vs.pt --fs ./scratch/data/{data}_fs.pt --init ./scratch/data/{data}_{init}.pt --savedir ./scratch/{expname}\"\n",
    "            if grad:\n",
    "                expstr += f\" --grad {grad}\"\n",
    "            f.write(expstr + \"\\n\")\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flat sanity check experiments\n",
    "basestr = [\"--experiment_type\", \"DEFAULT\", \"--size_train\", \"1\", \"--size_test\", \"1\", \"--epochs\", \"2000\",\n",
    "             \"--val_interval\", \"20\", \"--data_file\", \"cylinder.json\", \"--align_2D\", \"--xp_type\", \"uv\", \"--gpu_strategy\", \"ddp\",\n",
    "              \"--n_gpu\", \"1\", \"--no_poisson\", \"--identity\", \"--lr\", \"1e-5\",\n",
    "              \"--layer_normalization\", \"FLATTEN\",\n",
    "              \"--lossdistortion\", \"dirichlet\", \"--targets_per_batch\", \"16\",\n",
    "              \"--accumulate_grad_batches\", \"1\"]\n",
    "basestr = \"python ./training_scripts/train.py \" + ' '.join(basestr)\n",
    "\n",
    "datadirs = ['cylinder', 'cylinder_nocut']\n",
    "inits = ['isometric', 'tutte']\n",
    "sepdeltas = [0.1]\n",
    "# schedules = [True, False]\n",
    "grads = [None, 'l2', 'split']\n",
    "\n",
    "f = open(\"slurm/cylinder_flat\", 'w')\n",
    "for data in datadirs:\n",
    "    for delta in sepdeltas:\n",
    "        for init in inits:\n",
    "            for grad in grads:\n",
    "                expname = f\"flatcyl_sepd{delta}_data{data}_init{init}_grad{grad}\"\n",
    "                expstr = basestr + f\" --root_dir_train ./data/{data} --root_dir_test ./data/{data} --init {init} --seplossdelta {delta} --expname {expname}\"\n",
    "                if grad:\n",
    "                    expstr += f\" --lossgradientstitching {grad}\"\n",
    "                else:\n",
    "                    expstr += f\" --lossedgeseparation --seplossweight 1\"\n",
    "                f.write(expstr + \"\\n\")\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "basestr = [\"--root_dir_train\", \"./data/cylinder\", \"--root_dir_test\", \"./data/cylinder\",\n",
    "             \"--experiment_type\", \"DEFAULT\", \"--size_train\", \"1\", \"--size_test\", \"1\", \"--epochs\", \"8000\",\n",
    "             \"--val_interval\", \"20\", \"--data_file\", \"cylinder.json\", \"--align_2D\", \"--xp_type\", \"uv\", \"--gpu_strategy\", \"ddp\",\n",
    "              \"--n_gpu\", \"1\", \"--no_poisson\", \"--identity\", \"--lr\", \"0.005\",\n",
    "              \"--lossdistortion\", \"dirichlet\", \"--lossedgeseparation\", \"--seplossweight\", \"1\", \"--targets_per_batch\", \"16\",\n",
    "              \"--accumulate_grad_batches\", \"1\", \"--fft\", \"--fft_dim\", \"256\"]\n",
    "basestr = \"python ./training_scripts/train.py \" + ' '.join(basestr)\n",
    "sepdeltas = [0.01, 0.1, 0.5]\n",
    "seplosses = ['l1', 'mse']\n",
    "\n",
    "f = open(\"slurm/cylinder_exps\", 'w')\n",
    "for delta in sepdeltas:\n",
    "    for seploss in seplosses:\n",
    "        expname = f\"cylinder_sepd{delta}_sepl{seploss}\"\n",
    "        expstr = basestr + f\" --init isometric --eseploss {seploss} --seplossdelta {delta} --expname {expname}\"\n",
    "        f.write(expstr + \"\\n\")\n",
    "f.close()\n",
    "\n",
    "inits = ['isometric', 'tutte']\n",
    "f = open(\"slurm/cylinder_exps_v2\", 'w')\n",
    "for init in inits:\n",
    "    for seploss in seplosses:\n",
    "        expname = f\"cylinder_init{init}_sepl{seploss}\"\n",
    "        expstr = basestr + f\" --init {init} --eseploss {seploss} --expname {expname} --seploss_schedule --seplossdelta_min 0.001\"\n",
    "        f.write(expstr + \"\\n\")\n",
    "f.close()\n",
    "\n",
    "# Same for bunny\n",
    "basestr = [\"--root_dir_train\", \"./data/bunny\", \"--root_dir_test\", \"./data/bunny\",\n",
    "             \"--experiment_type\", \"DEFAULT\", \"--size_train\", \"1\", \"--size_test\", \"1\", \"--epochs\", \"8000\",\n",
    "             \"--val_interval\", \"20\", \"--data_file\", \"bunny.json\", \"--align_2D\", \"--xp_type\", \"uv\", \"--gpu_strategy\", \"ddp\",\n",
    "              \"--n_gpu\", \"1\", \"--no_poisson\", \"--identity\", \"--init\", \"isometric\", \"--lr\", \"0.005\",\n",
    "              \"--lossdistortion\", \"dirichlet\", \"--lossedgeseparation\", \"--seplossweight\", \"1\", \"--targets_per_batch\", \"16\",\n",
    "              \"--accumulate_grad_batches\", \"1\"]\n",
    "basestr = \"python ./training_scripts/train.py \" + ' '.join(basestr)\n",
    "sepdeltas = [0.01, 0.1, 0.5]\n",
    "seplosses = ['l1', 'mse']\n",
    "\n",
    "f = open(\"slurm/bunny_exps\", 'w')\n",
    "for delta in sepdeltas:\n",
    "    for seploss in seplosses:\n",
    "        expname = f\"bunny_sepd{delta}_sepl{seploss}\"\n",
    "        expstr = basestr + f\" --eseploss {seploss} --seplossdelta {delta} --expname {expname}\"\n",
    "        f.write(expstr + \"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--root_dir_train ./data/cylinder --root_dir_test ./data/cylinder --expname Debug --experiment_type DEFAULT --size_train 1 --size_test 1 --epochs 5000 --val_interval 20 --data_file cylinder.json --align_2D --xp_type uv --gpu_strategy ddp --n_gpu 1 --no_poisson --lossdistortion dirichlet --identity --init isometric --lossedgeseparation --seplossdelta 0.5 --seplossweight 1 --targets_per_batch 16 --accumulate_grad_batches 1\n"
     ]
    }
   ],
   "source": [
    "argstr = \"\"\"--root_dir_train ./data/cylinder --root_dir_test ./data/cylinder --expname Debug --experiment_type DEFAULT\n",
    "        --size_train 1 --size_test 1 --epochs 5000 --val_interval 20 --data_file cylinder.json --align_2D --xp_type uv\n",
    "        --gpu_strategy ddp --n_gpu 1 --no_poisson --lossdistortion dirichlet --identity --init isometric --lossedgeseparation\n",
    "        --seplossdelta 0.1 --seplossweight 1 --targets_per_batch 16 --accumulate_grad_batches 1\"\"\"\n",
    "print(\" \".join(argstr.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e6aba07be85c1c9a493ebf9e24b9687878e9ec4d611dd45a562119e3e9c612ed"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
